常用命令
	linux开启全部进程
		start-all.sh 
	linux开启(dfs,yarn)
		start-dfs.sh
		start-yarn.sh
	mapreduce 执行jar 包
		hadoop jar [*].jar [class]
		(2) mapreduce 指定输入输出路径
			/filePath  直接指定hdfs路径
	spark 执行jar包 注意jar包放在main参数最后
		spark-submit –-class [class*] [*].jar
		(2) spark 指定输入输出路径
			hdfs://master[主机名]:9000/filePath  前加hdfs端口 指定hdfs路径
	全局查找文件
		find [filename]
	hdfs 查看文件头
		hdfs dfs -cat /filename | head –[I int]	
	hdfs 查看行数
		hdfs dfs -cat /filename | wc -l
	hdfs 查看末尾
		hdfs dfs -tail /filename 
	ssh 通过秘钥主从传输
	
	hive 执行文件
		hive -f [filename*]
	linux -> hdfs
		hdfs dfs -put [linuxfile*] [hdfsfile*]
	hdfs -> linux
		hdfs dfs -get [hdfsfile*] [linuxfile*]
	scp 主从节点传输
		scp –r [/filePath*] root@192.168.3.100:[filePath]
	ssh 切换主从节点
		ssh [master*] || [192.*]
	授予权限
		hdfs dfs -chmod 777 /filenam                                                                                                                                                                  od  777  /【hdfs：path】   hdfs 设置全部用户可读可写可执行

	zip -d *.jar META-INF/*.RSA META-INF/*.SF

	return this.rank>o.getRank()?-1:1;  SortBean降序排序
	
	
	
恐怖错误！
	java.lang.SecurityException: Invalid signature file digest for Manifest main attributes
	在打jar包的时候，由于某些包的重复引用，以至于打包之后的META-INF的目录下多出了一些*.SF,*.DSA,*.RSA文件所致
	
	解决办法: 删除对应的jar 包下的文件
		zip -d *.jar META-INF/*.RSA META-INF/*.SF

Sqoop

	1．	Hive-mysql
	./sqoop export 
	--connect jdbc:mysql://ip地址:3306/库 \
	--username root \
	--passwrd 123456 \
	--table 表名 \
	--export-dir /hive路径  \
	--fields-terminated-by ‘,’


	2.mysql-hdfs
	./sqoop import \
	--connect jdbc:mysql://ip地址 \
	--username root \
	--passowrd 123456 \
	--table 表 \
	--target-dir fs路径 \（必须是新的地址）
	--m 1


	3.mysql-hive
	./sqoop import \
	--connect jdbc:mysql://ip地址 \
	--username root \
	--password 123456 \
	--table 表 \
	--fields-terminated-by ‘,’ \
	--delete-target-dir \
	--num-mappers 1 \
	--hive-import \
	--hive-database 库 \
	--hive-table 表